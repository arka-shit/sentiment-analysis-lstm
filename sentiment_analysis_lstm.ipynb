{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec970ab3-d66f-4865-98cc-d80e3a3a4acf",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    Sentiment Analysis with LSTM\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4737fe-dce7-4dd4-a2fd-a858fcccc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm, trange\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import gen_batches, shuffle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eeff22-55cf-460c-8386-f599b53f4851",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Setting the plotly renderer to iframe so that interactive plots show up correctly in nbviewer\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae086b1-1e8b-4812-a611-90b5da00630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e201c-e98c-4ca2-9d42-901d56b809f3",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    We are using the Yelp Dataset for our Sentiment Analysis\n",
    "</h2>\n",
    "<h4>\n",
    "    Loading the Yelp restaurant review dataset downloaded from kaggle. We are interested in only two columns \"stars\" and \"text\". \"stars\" column contain ratings given on a scale of 5 and \"text\" column contains the actual review text.\n",
    "    <br>\n",
    "    Also since this is a very big dataset we will be using a part of the dataset as sample for our analysis.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2900e182-8cb7-47ac-bf52-a019b1f6d56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4528116</th>\n",
       "      <td>3</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097267</th>\n",
       "      <td>5</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290314</th>\n",
       "      <td>3</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146971</th>\n",
       "      <td>3</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184541</th>\n",
       "      <td>3</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371430</th>\n",
       "      <td>4</td>\n",
       "      <td>Great sports bar with great bar food. The wing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572295</th>\n",
       "      <td>1</td>\n",
       "      <td>I went to Heart Attack Grill after seeing it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811562</th>\n",
       "      <td>4</td>\n",
       "      <td>Island Flavor is just as ono as the one on Dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767168</th>\n",
       "      <td>5</td>\n",
       "      <td>Five stars for our dinner service last night! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195319</th>\n",
       "      <td>5</td>\n",
       "      <td>Had a blast!  My girlfriend and I visited Vert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stars                                               text\n",
       "4528116      3  Airport Wendy's. You curbed my hunger. That wa...\n",
       "3097267      5  I stumbled across this store on my way to Nest...\n",
       "2290314      3  Pizza was decent. Very disappointed in the del...\n",
       "1146971      3  My first time: the bartenders were so cute [an...\n",
       "3184541      3  I was in las vegas staying at the Paris hotel ...\n",
       "...        ...                                                ...\n",
       "371430       4  Great sports bar with great bar food. The wing...\n",
       "1572295      1  I went to Heart Attack Grill after seeing it i...\n",
       "1811562      4  Island Flavor is just as ono as the one on Dur...\n",
       "3767168      5  Five stars for our dinner service last night! ...\n",
       "4195319      5  Had a blast!  My girlfriend and I visited Vert...\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"yelp_review.csv\",usecols=[\"stars\",\"text\"]).sample(300000,random_state=42)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c01106-aadf-4b02-b5f6-8b68c45472b5",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Mapping the stars to sentiment. For our analysis we will be using 3 levels of sentiment which are {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    <br>\n",
    "    For our analysis Reviews with 3 rating are considered \"neutral\" and anything above is \"positive\" and below is \"negative\"\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405a9bc9-312f-484f-8f9c-61e4163bdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stars_to_sentiment(stars):\n",
    "    if stars < 3:\n",
    "        return 0\n",
    "    elif stars == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1569f77-a2bb-475b-bf24-2d80b23e53be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4528116</th>\n",
       "      <td>3</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097267</th>\n",
       "      <td>5</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290314</th>\n",
       "      <td>3</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146971</th>\n",
       "      <td>3</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184541</th>\n",
       "      <td>3</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371430</th>\n",
       "      <td>4</td>\n",
       "      <td>Great sports bar with great bar food. The wing...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572295</th>\n",
       "      <td>1</td>\n",
       "      <td>I went to Heart Attack Grill after seeing it i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811562</th>\n",
       "      <td>4</td>\n",
       "      <td>Island Flavor is just as ono as the one on Dur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767168</th>\n",
       "      <td>5</td>\n",
       "      <td>Five stars for our dinner service last night! ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195319</th>\n",
       "      <td>5</td>\n",
       "      <td>Had a blast!  My girlfriend and I visited Vert...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stars                                               text  sentiment\n",
       "4528116      3  Airport Wendy's. You curbed my hunger. That wa...          1\n",
       "3097267      5  I stumbled across this store on my way to Nest...          2\n",
       "2290314      3  Pizza was decent. Very disappointed in the del...          1\n",
       "1146971      3  My first time: the bartenders were so cute [an...          1\n",
       "3184541      3  I was in las vegas staying at the Paris hotel ...          1\n",
       "...        ...                                                ...        ...\n",
       "371430       4  Great sports bar with great bar food. The wing...          2\n",
       "1572295      1  I went to Heart Attack Grill after seeing it i...          0\n",
       "1811562      4  Island Flavor is just as ono as the one on Dur...          2\n",
       "3767168      5  Five stars for our dinner service last night! ...          2\n",
       "4195319      5  Had a blast!  My girlfriend and I visited Vert...          2\n",
       "\n",
       "[300000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"] = df.apply(lambda x: stars_to_sentiment(x[\"stars\"]),axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fa538-649a-4259-b938-24f67cb54e87",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    By plotting our Sentiment Values we can see clearly see there is high class imbalance in our data.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ba79e3-1ae8-4854-9d71-c1af1144f654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "2    198149\n",
       "0     66851\n",
       "1     35000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fead60e-f21e-4ed6-9739-6656b54fbae3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    This is not good for any classification problem. So, Let's focus on balancing our data in the next step.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692b6172-e1b4-4f0e-b8ac-5d5cc6a4e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_16.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.histogram(df, x = \"sentiment\", color = \"sentiment\",width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde67926-80b8-4d0b-bcbc-9dc2376135c3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    One way to solve class imbalance is by under-sampling the majority classes. This strategy works well for large datasets (Which is true for our Large Yelp Dataset).\n",
    "    <br>\n",
    "    I have used RandomUnderSampler from imblearn to resample our dataset.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01140c13-c307-4b9e-81f4-52ff71da3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = RandomUnderSampler(random_state=42).fit_resample(df[[\"text\"]],df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfec831-a354-46dc-b1de-8df55aace4ff",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Post-undersampling, we can see that our dataset now has equal data for all classes. This gives the model an equal opportunity to learn the relationships between the data of each class and removes any naive classification validation error.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e351d5f-0f72-4dd2-b524-d23c5f76fc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    35000\n",
       "1    35000\n",
       "2    35000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb657668-cfc4-4e84-873a-6935fd078181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_21.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.histogram(Y, x = \"sentiment\", color = \"sentiment\",width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960df64-b658-4c65-8c09-4b9bfca8a3c5",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Next step is to preprocess the review text. I have used simple_preprocess from gensim for the first step. This lowers the text, removes punctuations, deaccentizes and also tokenize the text.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7247e7d-448a-4803-a319-ce69e83a355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893361    [was, in, las, vegas, with, some, friends, las...\n",
       "5247916    [should, have, done, some, research, or, looke...\n",
       "56197      [this, place, was, great, our, fam, night, eve...\n",
       "14430      [thanksgiving, dinner, was, so, much, better, ...\n",
       "3238309    [just, had, to, take, to, some, kind, of, soci...\n",
       "                                 ...                        \n",
       "4645274    [if, you, want, fine, dining, this, place, isn...\n",
       "4813124    [this, was, celebration, dinner, and, it, tota...\n",
       "2513110    [the, food, is, amazing, the, garlic, ramen, w...\n",
       "1994163    [love, this, place, not, only, do, we, buy, al...\n",
       "1846833    [came, in, for, lunch, and, had, an, amazing, ...\n",
       "Length: 105000, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.apply(lambda x: simple_preprocess(x[\"text\"],deacc=True),axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7f457-5e53-470c-a6cf-bdf708d4e2a3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Next I have lemmatize the tokens to get the root form. Thus words with similar root forms will not create separate tokens.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6fdb686-afb2-4a79-8431-b8aa04807b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "X = X.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e2350-8582-42eb-b0df-159ace131d2a",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Created the vocabulary. Also specified few special tokens to be used to signal our model about start of text, end of text, unknown word (not in current vocabulary), and padding tokens.\n",
    "    <br>\n",
    "    Set the max_tokens to 10k. So that our model can focus more on the most frequent words to understand their relationship and get less distracted.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ef7073f-97d0-486b-9cf8-a5ba9052e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '<pad>'\n",
    "start_token = '<sos>'\n",
    "end_token = '<eos>'\n",
    "unknown_yoken = '<unk>'\n",
    "max_tokens = 10000\n",
    "vocab = build_vocab_from_iterator(X,min_freq=2,specials=[pad_token,start_token,end_token,unknown_yoken],special_first=True,max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c77d6817-7227-4080-b7ae-f08c30878e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<sos>', '<eos>', '<unk>', 'the', 'and', 'to', 'wa', 'it', 'of']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce34d6-2638-4571-b384-72058445eefc",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Setting the default index to unknown. Thus by default, any out-of-vocabulary word will get that default unknown token.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bd4327d-b36a-4722-b3f5-1438f9b8b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[unknown_yoken])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdadd24a-b34d-4ef2-8935-830619703f6e",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating device-agnostic code. PyTorch will automatically use the GPU or CPU as per availability.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f7e00c3-e6e8-4485-9de7-3832a62db4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa5224-e245-437d-ae19-af2fe2420917",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating the label tensor\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74a5087f-be8b-4c4c-9820-b21e911e37ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 2, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor(Y.to_numpy(),device=device)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfd958-ce28-4e94-9e53-9bd0c036d118",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Splitting our data in training and testing set\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "149ab4f7-d752-46d1-a00e-a2c5c17fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X.to_list(),Y,random_state=42,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77bff167-3583-4a02-a704-0c604634b7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5d1b120-2875-4147-b656-6948cb284cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74be5c34-25f4-4357-80fa-b307e8556e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c6e87d5-d3ba-4b23-b90d-89ffda49049b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31500"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982d51c-0841-48ee-a49e-f625ffe40a28",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating our Text Transformation sequence.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63d8e85d-982e-40cb-8543-62395b66ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 512\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    # Convert the sentences to indices based on the given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add start_token at the beginning of each sentence.\n",
    "    T.AddToken(vocab[start_token], begin=True),\n",
    "    # Crop the sentence if it is longer than the specified max length\n",
    "    T.Truncate(max_seq_len=max_sequence_len),\n",
    "    # Add end_token at the end of each sentence.\n",
    "    T.AddToken(vocab[end_token], begin=False),\n",
    "    # Convert the list of lists to a tensor. This also pads a sentence with the pad_token if it is shorter than the max document length of the current batch, Thus ensuring that all sentences are the same length.\n",
    "    T.ToTensor(padding_value=vocab[pad_token])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532f938-d9fd-414d-83cd-7a741438aaba",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Defining the Neural Network Class. It consists of below layers --\n",
    "    <h4>\n",
    "    <ol>\n",
    "        <li>Embedding Layer - To create word embedding for our vocabulary.</li>\n",
    "        <li>LSTM Layers - We have used LSTM Layers for our Sentiment Analysis. It's good for NLP or any other sequential data like Speech Recognition or Time Series Data.</li>\n",
    "        <li>Fully Connected Layer - Finally a Linear Fully Connected Layer at the end.</li>\n",
    "    </ol>\n",
    "        Also added a helper function called init_hidden which will initialize the hidden and memory tensors on demand as per provided batch_size and available device\n",
    "    </h4>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3de26bc-d1d2-4691-91b3-c41f6f48aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, num_lstm_layers, hidden_dim, output_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim, num_layers=self.num_lstm_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(self.hidden_dim,output_dim)\n",
    "    \n",
    "    def forward(self,input_batch,hidden_in,mem_in):\n",
    "        output = self.embed(input_batch)\n",
    "        output, _ = self.lstm(output,(hidden_in,mem_in))\n",
    "        return self.fc(output)\n",
    "\n",
    "    def init_hidden(self,batch_size,device):\n",
    "        hidden = torch.zeros(self.num_lstm_layers,batch_size,self.hidden_dim).to(device)\n",
    "        memory = torch.zeros(self.num_lstm_layers,batch_size,self.hidden_dim).to(device)\n",
    "        return (hidden, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e74ee8-e439-44d7-92c5-ead2ba3ef138",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Initialized our LSTM Model with some carefully tuned hyperparameters obtained after several experimental runs. This provided good accuracy as we are going to see in later stages.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5590abca-3617-4461-95ed-e865990a010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentAnalysis(\n",
      "  (embed): Embedding(10000, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_lstm_layers = 2 \n",
    "embedding_dim = 64 \n",
    "hidden_dim = 256 \n",
    "output_dim = 3\n",
    "dropout = 0.5\n",
    "\n",
    "sentiment_classifier = SentimentAnalysis(vocab_size=len(vocab),embedding_dim=embedding_dim,num_lstm_layers=num_lstm_layers,hidden_dim=hidden_dim,output_dim=output_dim,dropout=dropout).to(device)\n",
    "\n",
    "print(sentiment_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2c436-10ea-4d24-8608-23d207ce95e7",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    For Loss Function we have used the Cross Entropy Loss and Optimizer we have used Adam optimzer which works very well with LSTM models.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c981f604-29f1-4fe3-a0fc-2964425b4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "# Used the default learning rate of 0.001 which provided good result in our analysis\n",
    "optimizer = Adam(sentiment_classifier.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecad775-68ed-4b27-9f23-05e164857dce",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    We already have approx 15 lakh parameters in our model. Which are going to be tuned in the training epochs.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "810412ce-5ce8-431b-8cf2-84de3ca14df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model Has 1496835 (Approximately 14.97 Lakhs) Parameters!\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in sentiment_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(f\"This Model Has {num_model_params} (Approximately {round(num_model_params/100000,2)} Lakhs) Parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1587a3-5a29-432a-9d63-b2a4f803e0e1",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Using the gen_batches from sklearn to create our batch slices which are going to be used in later stages to train our model in batches.\n",
    "    <br>\n",
    "    Using an optimal batch size is important for LSTM models. For our analysis we found batches of 64 works well.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32024ec7-6332-4309-9bc9-22f13a2d64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_batches = list(gen_batches(len(train_y),batch_size))\n",
    "test_batches = list(gen_batches(len(test_y),batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf88e32-67ab-4b73-9881-fa6c8363aa4f",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Initializing Train and Test Loss and Accuracy loggers, which are going to be used later to plot and track our model progress.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d17f8912-d730-47cf-8fce-8148a201046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_logger = list()\n",
    "test_loss_loger = list()\n",
    "train_accuracy_logger = list()\n",
    "test_accuracy_logger = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcb483-c433-42c6-a94e-f03a8d6e920b",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Training and Testing in batches for several epochs. Number epochs to run is important for getting good accuracy. We will train our model till we achieve good accuracy.\n",
    "</h3>\n",
    "<h4>\n",
    "    I have also used tqdm progress bar with insight full postfix to track our model progress on the go.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7f2d7e7e-da7a-4b8d-9b0f-cf3200f87ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f3697259894c32b31eb0db8265705f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arkas\\anaconda3\\Lib\\site-packages\\torch\\_jit_internal.py:1358: UserWarning:\n",
      "\n",
      "The inner type of a container is lost when calling torch.jit.isinstance in eager mode. For example, List[int] would become list and therefore falsely return True for List[float] or List[str].\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_epochs = 9\n",
    "clip = 5\n",
    "\n",
    "pbar = trange(total_epochs,desc=\"Epoch\")\n",
    "\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "cur_train_loss = 0\n",
    "cur_test_loss = 0\n",
    "\n",
    "for epoch in pbar:\n",
    "\n",
    "    # Shuffling the Train and Test Dataset after each epoch for better training and validation\n",
    "    \n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    test_x, test_y = shuffle(test_x, test_y)\n",
    "    \n",
    "    pbar.set_postfix_str(f\"Train Accuracy: {round(train_acc*100,2)}% | Test Accuracy: {round(test_acc*100,2)}% | Train Loss: {round(cur_train_loss,4)} | Test Loss: {round(cur_test_loss,4)}\")\n",
    "    sentiment_classifier.train()\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_losses = list()\n",
    "    for train_batch in tqdm(train_batches,desc=\"Training in Batches\",leave=False):\n",
    "        text = train_x[train_batch]\n",
    "        label_tensor = train_y[train_batch]\n",
    "        text_tensor = text_transform(text).to(device)\n",
    "        \n",
    "        hidden, memory = sentiment_classifier.init_hidden(len(label_tensor),device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = sentiment_classifier(text_tensor,hidden,memory)\n",
    "\n",
    "        loss = loss_fun(pred[:,-1,:],label_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # 'clip_grad_norm' helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(sentiment_classifier.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        train_acc += (pred[:,-1,:].argmax(1) == label_tensor).sum()\n",
    "\n",
    "    cur_train_loss = np.mean(train_losses)\n",
    "    train_loss_logger.append(cur_train_loss)\n",
    "    \n",
    "    train_acc = (train_acc/len(train_y)).item()\n",
    "    train_accuracy_logger.append(train_acc)\n",
    "\n",
    "\n",
    "    sentiment_classifier.eval()\n",
    "    test_acc = 0\n",
    "    test_losses = list()\n",
    "    with torch.inference_mode():\n",
    "        for test_batch in tqdm(test_batches,desc=\"Testing in Batches\",leave=False):\n",
    "            text = test_x[test_batch]\n",
    "            label_tensor = test_y[test_batch]\n",
    "\n",
    "            text = text_transform(text).to(device)\n",
    "\n",
    "            hidden, memory = sentiment_classifier.init_hidden(len(label_tensor),device)\n",
    "            \n",
    "            pred = sentiment_classifier(text,hidden,memory)\n",
    "\n",
    "            loss = loss_fun(pred[:,-1,:],label_tensor)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            test_acc += (pred[:,-1,:].argmax(1) == label_tensor).sum()\n",
    "\n",
    "    cur_test_loss = np.mean(test_losses)\n",
    "    test_loss_loger.append(cur_test_loss)\n",
    "    test_acc = (test_acc/len(test_y)).item()\n",
    "    test_accuracy_logger.append(test_acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e9333298-f98a-4f77-8126-51bcb1db8f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.094891</td>\n",
       "      <td>1.087129</td>\n",
       "      <td>0.353361</td>\n",
       "      <td>0.389365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.094044</td>\n",
       "      <td>1.095253</td>\n",
       "      <td>0.352313</td>\n",
       "      <td>0.340127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.092344</td>\n",
       "      <td>1.083762</td>\n",
       "      <td>0.354286</td>\n",
       "      <td>0.398127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.073723</td>\n",
       "      <td>1.062219</td>\n",
       "      <td>0.397088</td>\n",
       "      <td>0.402032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.868909</td>\n",
       "      <td>0.716861</td>\n",
       "      <td>0.584735</td>\n",
       "      <td>0.679206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.646818</td>\n",
       "      <td>0.607740</td>\n",
       "      <td>0.720463</td>\n",
       "      <td>0.744794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.562180</td>\n",
       "      <td>0.576635</td>\n",
       "      <td>0.760422</td>\n",
       "      <td>0.757651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.514992</td>\n",
       "      <td>0.552405</td>\n",
       "      <td>0.781850</td>\n",
       "      <td>0.762349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.474975</td>\n",
       "      <td>0.550874</td>\n",
       "      <td>0.800639</td>\n",
       "      <td>0.772698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.439333</td>\n",
       "      <td>0.551570</td>\n",
       "      <td>0.818150</td>\n",
       "      <td>0.769365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Train Loss  Test Loss  Train Accuracy  Test Accuracy\n",
       "0       1    1.094891   1.087129        0.353361       0.389365\n",
       "1       2    1.094044   1.095253        0.352313       0.340127\n",
       "2       3    1.092344   1.083762        0.354286       0.398127\n",
       "3       4    1.073723   1.062219        0.397088       0.402032\n",
       "4       5    0.868909   0.716861        0.584735       0.679206\n",
       "5       6    0.646818   0.607740        0.720463       0.744794\n",
       "6       7    0.562180   0.576635        0.760422       0.757651\n",
       "7       8    0.514992   0.552405        0.781850       0.762349\n",
       "8       9    0.474975   0.550874        0.800639       0.772698\n",
       "9      10    0.439333   0.551570        0.818150       0.769365"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance = pd.DataFrame({\"Epochs\": list(range(1,len(train_loss_logger)+1)),\"Train Loss\": train_loss_logger,\"Test Loss\": test_loss_loger,\"Train Accuracy\": train_accuracy_logger, \"Test Accuracy\": test_accuracy_logger})\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdacc7-91cc-4ac6-a0f7-959318bff37d",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Plotting Train and Test Loss\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "14c8b424-1efe-492b-8e47-dacc41e466ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_120.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(model_performance,x=\"Epochs\",y=[\"Train Loss\",\"Test Loss\"],markers=True,height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392bb4a-6fd7-486d-8255-63ada0c45ab3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Plotting Train and Test Accuracy\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0f545d64-a179-4005-ab9a-cee3f49d9dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_122.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(model_performance,x=\"Epochs\", y = [\"Train Accuracy\",\"Test Accuracy\"],markers=True,height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4ef07-581e-4bdc-8817-9a6dc2cd932e",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    We are able to achieve 77% accuracy from our model\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "20a0ad81-ff77-4a7d-8639-9c88e26c9847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7726984620094299"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_accuracy_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27774c-062f-4e1e-b598-d456bb6042e9",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating a helper function to use our model to perform sentiment analysis\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "80366353-4b22-4cf0-8cdb-43412fe785b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review_data):\n",
    "    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    sentiment_classifier.eval()\n",
    "    with torch.inference_mode():\n",
    "        for index, row in review_data.iterrows():\n",
    "            orginal_sentiment = sentiment_map[stars_to_sentiment(row[\"stars\"])]\n",
    "            review_tokens = [lemmatizer.lemmatize(i) for i in simple_preprocess(row[\"text\"])]\n",
    "            review_tokens = np.expand_dims(review_tokens,axis=0).tolist()\n",
    "            review_tensor = text_transform(review_tokens).to(device)\n",
    "            hidden, memory = sentiment_classifier.init_hidden(1,device)\n",
    "            pred = sentiment_classifier(review_tensor,hidden,memory)\n",
    "            pred_sentiment = sentiment_map[pred[:,-1,:].argmax(1).item()]\n",
    "            pred_sentiment_probability = pred[:,-1,:].softmax(1).max().item()\n",
    "\n",
    "            print(\"Review Text:- \")\n",
    "            print(row[\"text\"])\n",
    "            print(\"========================================================================================================\")\n",
    "            print(f\"Rating:- {row[\"stars\"]}\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(f\"Actual Sentiment:- {orginal_sentiment}\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(f\"Predicted Sentimen:- {pred_sentiment}\")\n",
    "            print(f\"Prediction Probability:- {pred_sentiment_probability}\")\n",
    "            print(\"#########################################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabe9af-46ce-49a3-b7a3-bab4c3b09f80",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's fetch some completely unseen data and perform sentiment analysis\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "888a1be3-d97e-4bb6-9ec7-d4ac078ab3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv(\"yelp_review.csv\",usecols=[\"stars\",\"text\"]).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "828e8a10-8dde-4f44-8fd3-3074526c6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Text:- \n",
      "I really liked this place, as I got delivery from here often.  Usually takes an hour, give or take ten minutes.\n",
      "\n",
      "Then, one night, it took two hours.\n",
      "\n",
      "It wasn't a prime-time.  Around 9:15 pm on a Sunday night, not unusual timing for me.   I figured it'd arrive around 10:15, I'd eat, and be in bed by 11.\n",
      "\n",
      "I was wrong.\n",
      "\n",
      "Around 10:25, I called Thai One On and asked about my delivery.  \"It'll be there soon,\" was all I was told.  10:45 pm I called back.  \"Yeah the guy is close,\" I was told.  \n",
      "\n",
      "Everytime I would phone, no one would really answer me.  I ordered through JustEat, usually embedded in your confirmation email is a link to see the real-time of when the driver will be there.  It said 10:15 pm.  It was now nearing on 11 pm.  I wanted to be in bed.  I was hungry.  I was confused.  I just wanted answers.\n",
      "\n",
      "When my food arrived past 11:15, I called back one final time and the guy said someone from head management would be calling me the next day before 4 pm.  The next day came and went.  I emailed JustEat and asked what I should do.  (This service by the way was great, I mean, it wasn't their fault they are a middle man, but their customer service is really good.  You can live chat with an agent which is great because I could do it from my desk at work which still doing my work)  They called Thai One On for me and was told someone from management would phone me the next day.  The next day: came. Went.  I emailed JustEat and told them the issue and finally someone from Thai One On called me.\n",
      "\n",
      "I don't even know if the guy was speaking English, every now and again a word would come out that sounded like a word I recognized, but basically he was talking so fast with such a strong accent I have no idea what the agreement or outcome was.  I never got answers.  I would have been happy with \"sorry\".  At the end he finally said he would text me a code I could use on their personal website for 25% off.  I never heard from him again.  \n",
      "\n",
      "I liked the food, but I can't handle the run around.\n",
      "========================================================================================================\n",
      "Rating:- 1\n",
      "========================================================================================================\n",
      "Actual Sentiment:- negative\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- negative\n",
      "Prediction Probability:- 0.9465630054473877\n",
      "#########################################################################################################\n",
      "Review Text:- \n",
      "The waiter lied and said we couldn't use are credit cards just because we ordered rice plates instead of entrees. Excuse me, where does it say that you have to pay with cash if you order rice plates? The total bill was over $20. The policy is that credit cards are accepted if the total bill is at least $20. What a BS service! Do not come to this restaurant if you plan to use your credit card!\n",
      "========================================================================================================\n",
      "Rating:- 1\n",
      "========================================================================================================\n",
      "Actual Sentiment:- negative\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- negative\n",
      "Prediction Probability:- 0.9349951148033142\n",
      "#########################################################################################################\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(testing_data[testing_data[\"stars\"] < 3].sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8f5ef8c3-e261-44b3-a86d-3c896411d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Text:- \n",
      "Not bad at all.  \n",
      "\n",
      "We've been on a bit of a pho-kick recently, so we've been trying a few new places.  Pho Ao Sen is right down the road from our house, so it's the closest one we've tried.  We had dinner there once, and we've done take-out Pho a handful of other times.\n",
      "\n",
      "When we dined it, they were a bit busy, and appeared understaffed.  They were pleasant enough, and certainly worked hard and fast to get everyone taken care of quickly.\n",
      "\n",
      "The pho was pretty good.  It's on par with most other places in the valley.  Good broth, I like that they use cilantro and green onion (which some places seem to skimp on -- just my preference to have more in there).  \n",
      "\n",
      "The spring rolls were mediocre.  They fell apart pretty easily and the quality was just OK.  Not bad, but I've definitely had better.\n",
      "\n",
      "The Vietnamese iced coffee was fantastic.  It's so simple, strong as hell and sweet.  It was great to end dinner on.  (For the record, my wife didn't care for it - it was a bit strong for her tastes).  \n",
      "\n",
      "The few times we've done takeout, the pho has held very well - they give you the broth separate from the ingredients and you assemble when you get home.  I was skeptical of how good take-out pho would be, but it worked quite well.  The broth was still hot when we got home and it had a very similar quality whether I ate it at the restaurant or at home on the couch.  \n",
      "\n",
      "We'll definitely return, but it's not my favorite pho in Phoenix.  It's just SO close to home.\n",
      "========================================================================================================\n",
      "Rating:- 3\n",
      "========================================================================================================\n",
      "Actual Sentiment:- neutral\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- neutral\n",
      "Prediction Probability:- 0.8156026005744934\n",
      "#########################################################################################################\n",
      "Review Text:- \n",
      "Truely a 3.5 star shop!  Very big shop with plenty of free street parking up front.   The staff here are very friendly and accommodating to special requests within reason. \n",
      "They run a tight ship here and I find the service here quicker than your average pizza pizza elsewhere.\n",
      "No true store only specials but if you ask they may be v able to do a little extra with the friendly crew.  \n",
      "\n",
      "Too bad the coffee shop that was across closed up shop or it would have been perfect with a nice coffee with a freshly made pie.  Will be visiting again shortly when the cravings return\n",
      "========================================================================================================\n",
      "Rating:- 3\n",
      "========================================================================================================\n",
      "Actual Sentiment:- neutral\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- neutral\n",
      "Prediction Probability:- 0.7949254512786865\n",
      "#########################################################################################################\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(testing_data[testing_data[\"stars\"] == 3].sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d1666f9e-b1ae-48f5-85ec-b4f81efd95b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Text:- \n",
      "Ordered a large pizza with everything from this pizzeria and was very happy with the taste and the quality of the pizza. I went to pickup the pie and the place is super cute with lots of TVs on the wall and very friendly staff. I will definitely be back!\n",
      "========================================================================================================\n",
      "Rating:- 4\n",
      "========================================================================================================\n",
      "Actual Sentiment:- positive\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- positive\n",
      "Prediction Probability:- 0.928472101688385\n",
      "#########################################################################################################\n",
      "Review Text:- \n",
      "The Cambridge is the home of the most famous burgers in Edinburgh, a reputation that is well deserved. The burgers are excellent and the bar itself is great with a good selection of drinks and a nice atmosphere. If you don't like burgers then don't go there because that's all they sell! The bar is small so it's best to book a table if you want to eat - they only take 5 bookings per evening (as they wish to operate more as a bar than a restaurant) so get in quick! In all, this is one of the best places in Edinburgh to get a relaxed meal and has a great atmosphere every night of the week, being particularly busy when big football matches are on due to the big screens.\n",
      "========================================================================================================\n",
      "Rating:- 5\n",
      "========================================================================================================\n",
      "Actual Sentiment:- positive\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- positive\n",
      "Prediction Probability:- 0.9775856733322144\n",
      "#########################################################################################################\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(testing_data[testing_data[\"stars\"] > 3].sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ef8ed-95e5-439e-a8a8-f4e39764a77d",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Finally with above result we can conclude our analysis. Our model was able to get good accuracy on the testing dataset and also able to correctly identify the sentiment of the unseen datas.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a48d5-06e3-4b39-853c-1dde70a9ec7e",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Saving our trained model for future use.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a89cd6e7-f4c3-4d08-92e3-8c33a4786760",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sentiment_classifier,\"LSTM_Sentiment_Classifier.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
