{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec970ab3-d66f-4865-98cc-d80e3a3a4acf",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    Sentiment Analysis with LSTM\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4737fe-dce7-4dd4-a2fd-a858fcccc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm, trange\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import gen_batches, shuffle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eeff22-55cf-460c-8386-f599b53f4851",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Setting the plotly renderer to iframe so that interactive plots show up correctly in nbviewer\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae086b1-1e8b-4812-a611-90b5da00630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e201c-e98c-4ca2-9d42-901d56b809f3",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    We are using the Yelp Dataset for our Sentiment Analysis\n",
    "</h2>\n",
    "<h4>\n",
    "    Loading the Yelp restaurant review dataset downloaded from kaggle. We are interested in only two columns \"stars\" and \"text\". \"stars\" column contain ratings given on a scale of 5 and \"text\" column contains the actual review text.\n",
    "    <br>\n",
    "    Also since this is a very big dataset we will be using a part of the dataset as sample for our analysis.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2900e182-8cb7-47ac-bf52-a019b1f6d56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4528116</th>\n",
       "      <td>3</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097267</th>\n",
       "      <td>5</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290314</th>\n",
       "      <td>3</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146971</th>\n",
       "      <td>3</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184541</th>\n",
       "      <td>3</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371430</th>\n",
       "      <td>4</td>\n",
       "      <td>Great sports bar with great bar food. The wing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572295</th>\n",
       "      <td>1</td>\n",
       "      <td>I went to Heart Attack Grill after seeing it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811562</th>\n",
       "      <td>4</td>\n",
       "      <td>Island Flavor is just as ono as the one on Dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767168</th>\n",
       "      <td>5</td>\n",
       "      <td>Five stars for our dinner service last night! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195319</th>\n",
       "      <td>5</td>\n",
       "      <td>Had a blast!  My girlfriend and I visited Vert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stars                                               text\n",
       "4528116      3  Airport Wendy's. You curbed my hunger. That wa...\n",
       "3097267      5  I stumbled across this store on my way to Nest...\n",
       "2290314      3  Pizza was decent. Very disappointed in the del...\n",
       "1146971      3  My first time: the bartenders were so cute [an...\n",
       "3184541      3  I was in las vegas staying at the Paris hotel ...\n",
       "...        ...                                                ...\n",
       "371430       4  Great sports bar with great bar food. The wing...\n",
       "1572295      1  I went to Heart Attack Grill after seeing it i...\n",
       "1811562      4  Island Flavor is just as ono as the one on Dur...\n",
       "3767168      5  Five stars for our dinner service last night! ...\n",
       "4195319      5  Had a blast!  My girlfriend and I visited Vert...\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"yelp_review.csv\",usecols=[\"stars\",\"text\"]).sample(300000,random_state=42)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c01106-aadf-4b02-b5f6-8b68c45472b5",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Mapping the stars to sentiment. For our analysis we will be using 3 levels of sentiment which are {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "    <br>\n",
    "    For our analysis Reviews with 3 rating are considered \"neutral\" and anything above is \"positive\" and below is \"negative\"\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405a9bc9-312f-484f-8f9c-61e4163bdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stars_to_sentiment(stars):\n",
    "    if stars < 3:\n",
    "        return 0\n",
    "    elif stars == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1569f77-a2bb-475b-bf24-2d80b23e53be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4528116</th>\n",
       "      <td>3</td>\n",
       "      <td>Airport Wendy's. You curbed my hunger. That wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097267</th>\n",
       "      <td>5</td>\n",
       "      <td>I stumbled across this store on my way to Nest...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290314</th>\n",
       "      <td>3</td>\n",
       "      <td>Pizza was decent. Very disappointed in the del...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146971</th>\n",
       "      <td>3</td>\n",
       "      <td>My first time: the bartenders were so cute [an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184541</th>\n",
       "      <td>3</td>\n",
       "      <td>I was in las vegas staying at the Paris hotel ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371430</th>\n",
       "      <td>4</td>\n",
       "      <td>Great sports bar with great bar food. The wing...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572295</th>\n",
       "      <td>1</td>\n",
       "      <td>I went to Heart Attack Grill after seeing it i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811562</th>\n",
       "      <td>4</td>\n",
       "      <td>Island Flavor is just as ono as the one on Dur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767168</th>\n",
       "      <td>5</td>\n",
       "      <td>Five stars for our dinner service last night! ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195319</th>\n",
       "      <td>5</td>\n",
       "      <td>Had a blast!  My girlfriend and I visited Vert...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stars                                               text  sentiment\n",
       "4528116      3  Airport Wendy's. You curbed my hunger. That wa...          1\n",
       "3097267      5  I stumbled across this store on my way to Nest...          2\n",
       "2290314      3  Pizza was decent. Very disappointed in the del...          1\n",
       "1146971      3  My first time: the bartenders were so cute [an...          1\n",
       "3184541      3  I was in las vegas staying at the Paris hotel ...          1\n",
       "...        ...                                                ...        ...\n",
       "371430       4  Great sports bar with great bar food. The wing...          2\n",
       "1572295      1  I went to Heart Attack Grill after seeing it i...          0\n",
       "1811562      4  Island Flavor is just as ono as the one on Dur...          2\n",
       "3767168      5  Five stars for our dinner service last night! ...          2\n",
       "4195319      5  Had a blast!  My girlfriend and I visited Vert...          2\n",
       "\n",
       "[300000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"] = df.apply(lambda x: stars_to_sentiment(x[\"stars\"]),axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fa538-649a-4259-b938-24f67cb54e87",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    By plotting our Sentiment Values we can see clearly see there is high class imbalance in our data.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ba79e3-1ae8-4854-9d71-c1af1144f654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "2    198149\n",
       "0     66851\n",
       "1     35000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fead60e-f21e-4ed6-9739-6656b54fbae3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    This is not good for any classification problem. So, Let's focus on balancing our data in the next step.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692b6172-e1b4-4f0e-b8ac-5d5cc6a4e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_13.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.histogram(df, x = \"sentiment\", color = \"sentiment\",width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde67926-80b8-4d0b-bcbc-9dc2376135c3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    One way to solve class imbalance is by under-sampling the majority classes. This strategy works well for large datasets (Which is true for our Large Yelp Dataset).\n",
    "    <br>\n",
    "    I have used RandomUnderSampler from imblearn to resample our dataset.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01140c13-c307-4b9e-81f4-52ff71da3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = RandomUnderSampler(random_state=42).fit_resample(df[[\"text\"]],df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfec831-a354-46dc-b1de-8df55aace4ff",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Post-undersampling, we can see that our dataset now has equal data for all classes. This gives the model an equal opportunity to learn the relationships between the data of each class and removes any naive classification validation error.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e351d5f-0f72-4dd2-b524-d23c5f76fc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    35000\n",
       "1    35000\n",
       "2    35000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb657668-cfc4-4e84-873a-6935fd078181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.histogram(Y, x = \"sentiment\", color = \"sentiment\",width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960df64-b658-4c65-8c09-4b9bfca8a3c5",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Next step is to preprocess the review text. I have used simple_preprocess from gensim for the first step. This lowers the text, removes punctuations, deaccentizes and also tokenize the text.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7247e7d-448a-4803-a319-ce69e83a355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893361    [was, in, las, vegas, with, some, friends, las...\n",
       "5247916    [should, have, done, some, research, or, looke...\n",
       "56197      [this, place, was, great, our, fam, night, eve...\n",
       "14430      [thanksgiving, dinner, was, so, much, better, ...\n",
       "3238309    [just, had, to, take, to, some, kind, of, soci...\n",
       "                                 ...                        \n",
       "4645274    [if, you, want, fine, dining, this, place, isn...\n",
       "4813124    [this, was, celebration, dinner, and, it, tota...\n",
       "2513110    [the, food, is, amazing, the, garlic, ramen, w...\n",
       "1994163    [love, this, place, not, only, do, we, buy, al...\n",
       "1846833    [came, in, for, lunch, and, had, an, amazing, ...\n",
       "Length: 105000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.apply(lambda x: simple_preprocess(x[\"text\"],deacc=True),axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7f457-5e53-470c-a6cf-bdf708d4e2a3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Next I have lemmatize the tokens to get the root form. Thus words with similar root forms will not create separate tokens.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6fdb686-afb2-4a79-8431-b8aa04807b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "X = X.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e2350-8582-42eb-b0df-159ace131d2a",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Created the vocabulary. Also specified few special tokens to be used to signal our model about start of text, end of text, unknown word (not in current vocabulary), and padding tokens.\n",
    "    <br>\n",
    "    Set the max_tokens to 10k. So that our model can focus more on the most frequent words to understand their relationship and get less distracted.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ef7073f-97d0-486b-9cf8-a5ba9052e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '<pad>'\n",
    "start_token = '<sos>'\n",
    "end_token = '<eos>'\n",
    "unknown_yoken = '<unk>'\n",
    "max_tokens = 10000\n",
    "vocab = build_vocab_from_iterator(X,min_freq=2,specials=[pad_token,start_token,end_token,unknown_yoken],special_first=True,max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c77d6817-7227-4080-b7ae-f08c30878e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<sos>', '<eos>', '<unk>', 'the', 'and', 'to', 'wa', 'it', 'of']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce34d6-2638-4571-b384-72058445eefc",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Setting the default index to unknown. Thus by default, any out-of-vocabulary word will get that default unknown token.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd4327d-b36a-4722-b3f5-1438f9b8b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[unknown_yoken])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdadd24a-b34d-4ef2-8935-830619703f6e",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating device-agnostic code. PyTorch will automatically use the GPU or CPU as per availability.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f7e00c3-e6e8-4485-9de7-3832a62db4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa5224-e245-437d-ae19-af2fe2420917",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating the label tensor\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74a5087f-be8b-4c4c-9820-b21e911e37ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 2, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor(Y.to_numpy(),device=device)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfd958-ce28-4e94-9e53-9bd0c036d118",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Splitting our data in training and testing set\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "149ab4f7-d752-46d1-a00e-a2c5c17fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X.to_list(),Y,random_state=42,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77bff167-3583-4a02-a704-0c604634b7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73500"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5d1b120-2875-4147-b656-6948cb284cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73500"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74be5c34-25f4-4357-80fa-b307e8556e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c6e87d5-d3ba-4b23-b90d-89ffda49049b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982d51c-0841-48ee-a49e-f625ffe40a28",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating our Text Transformation sequence.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63d8e85d-982e-40cb-8543-62395b66ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 512\n",
    "\n",
    "text_transform = T.Sequential(\n",
    "    # Convert the sentences to indices based on the given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add start_token at the beginning of each sentence.\n",
    "    T.AddToken(vocab[start_token], begin=True),\n",
    "    # Crop the sentence if it is longer than the specified max length\n",
    "    T.Truncate(max_seq_len=max_sequence_len),\n",
    "    # Add end_token at the end of each sentence.\n",
    "    T.AddToken(vocab[end_token], begin=False),\n",
    "    # Convert the list of lists to a tensor. This also pads a sentence with the pad_token if it is shorter than the max document length of the current batch, Thus ensuring that all sentences are the same length.\n",
    "    T.ToTensor(padding_value=vocab[pad_token])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532f938-d9fd-414d-83cd-7a741438aaba",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Defining the Neural Network Class. It consists of below layers --\n",
    "    <h4>\n",
    "    <ol>\n",
    "        <li>Embedding Layer - To create word embedding for our vocabulary.</li>\n",
    "        <li>LSTM Layers - We have used LSTM Layers for our Sentiment Analysis. It's good for NLP or any other sequential data like Speech Recognition or Time Series Data.</li>\n",
    "        <li>Fully Connected Layer - Finally a Linear Fully Connected Layer at the end.</li>\n",
    "    </ol>\n",
    "        Also added a helper function called init_hidden which will initialize the hidden and memory tensors on demand as per provided batch_size and available device\n",
    "    </h4>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3de26bc-d1d2-4691-91b3-c41f6f48aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, num_lstm_layers, hidden_dim, output_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim, num_layers=self.num_lstm_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(self.hidden_dim,output_dim)\n",
    "    \n",
    "    def forward(self,input_batch,hidden_in,mem_in):\n",
    "        output = self.embed(input_batch)\n",
    "        output, _ = self.lstm(output,(hidden_in,mem_in))\n",
    "        return self.fc(output)\n",
    "\n",
    "    def init_hidden(self,batch_size,device):\n",
    "        hidden = torch.zeros(self.num_lstm_layers,batch_size,self.hidden_dim).to(device)\n",
    "        memory = torch.zeros(self.num_lstm_layers,batch_size,self.hidden_dim).to(device)\n",
    "        return (hidden, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e74ee8-e439-44d7-92c5-ead2ba3ef138",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Initialized our LSTM Model with some carefully tuned hyperparameters obtained after several experimental runs. This provided good accuracy as we are going to see in later stages.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5590abca-3617-4461-95ed-e865990a010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentAnalysis(\n",
      "  (embed): Embedding(10000, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_lstm_layers = 2 \n",
    "embedding_dim = 64 \n",
    "hidden_dim = 256 \n",
    "output_dim = 3\n",
    "dropout = 0.5\n",
    "\n",
    "sentiment_classifier = SentimentAnalysis(vocab_size=len(vocab),embedding_dim=embedding_dim,num_lstm_layers=num_lstm_layers,hidden_dim=hidden_dim,output_dim=output_dim,dropout=dropout).to(device)\n",
    "\n",
    "print(sentiment_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2c436-10ea-4d24-8608-23d207ce95e7",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    For Loss Function we have used the Cross Entropy Loss and Optimizer we have used Adam optimzer which works very well with LSTM models.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c981f604-29f1-4fe3-a0fc-2964425b4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "# Used the default learning rate of 0.001 which provided good result in our analysis\n",
    "optimizer = Adam(sentiment_classifier.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecad775-68ed-4b27-9f23-05e164857dce",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    We already have approx 15 lakh parameters in our model. Which are going to be tuned in the training epochs.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "810412ce-5ce8-431b-8cf2-84de3ca14df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model Has 1496835 (Approximately 14.97 Lakhs) Parameters!\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in sentiment_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(f\"This Model Has {num_model_params} (Approximately {round(num_model_params/100000,2)} Lakhs) Parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1587a3-5a29-432a-9d63-b2a4f803e0e1",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Using the gen_batches from sklearn to create our batch slices which are going to be used in later stages to train our model in batches.\n",
    "    <br>\n",
    "    Using an optimal batch size is important for LSTM models. For our analysis we found batches of 64 works well.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32024ec7-6332-4309-9bc9-22f13a2d64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_batches = list(gen_batches(len(train_y),batch_size))\n",
    "test_batches = list(gen_batches(len(test_y),batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf88e32-67ab-4b73-9881-fa6c8363aa4f",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Initializing Train and Test Loss and Accuracy loggers, which are going to be used later to plot and track our model progress.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d17f8912-d730-47cf-8fce-8148a201046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_logger = list()\n",
    "test_loss_loger = list()\n",
    "train_accuracy_logger = list()\n",
    "test_accuracy_logger = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcb483-c433-42c6-a94e-f03a8d6e920b",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Training and Testing in batches for several epochs. Number epochs to run is important for getting good accuracy. We will train our model till we achieve good accuracy.\n",
    "</h3>\n",
    "<h4>\n",
    "    I have also used tqdm progress bar with insight full postfix to track our model progress on the go.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f2d7e7e-da7a-4b8d-9b0f-cf3200f87ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff42f0693e645849bba13c884f3f104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arkas\\anaconda3\\Lib\\site-packages\\torch\\_jit_internal.py:1358: UserWarning:\n",
      "\n",
      "The inner type of a container is lost when calling torch.jit.isinstance in eager mode. For example, List[int] would become list and therefore falsely return True for List[float] or List[str].\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training in Batches:   0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing in Batches:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_epochs = 9\n",
    "clip = 5\n",
    "\n",
    "pbar = trange(total_epochs,desc=\"Epoch\")\n",
    "\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "cur_train_loss = 0\n",
    "cur_test_loss = 0\n",
    "\n",
    "for epoch in pbar:\n",
    "\n",
    "    # Shuffling the Train and Test Dataset after each epoch for better training and validation\n",
    "    \n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    test_x, test_y = shuffle(test_x, test_y)\n",
    "    \n",
    "    pbar.set_postfix_str(f\"Train Accuracy: {round(train_acc*100,2)}% | Test Accuracy: {round(test_acc*100,2)}% | Train Loss: {round(cur_train_loss,4)} | Test Loss: {round(cur_test_loss,4)}\")\n",
    "    sentiment_classifier.train()\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_losses = list()\n",
    "    for train_batch in tqdm(train_batches,desc=\"Training in Batches\",leave=False):\n",
    "        text = train_x[train_batch]\n",
    "        label_tensor = train_y[train_batch]\n",
    "        text_tensor = text_transform(text).to(device)\n",
    "        \n",
    "        hidden, memory = sentiment_classifier.init_hidden(len(label_tensor),device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = sentiment_classifier(text_tensor,hidden,memory)\n",
    "\n",
    "        loss = loss_fun(pred[:,-1,:],label_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # 'clip_grad_norm' helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(sentiment_classifier.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        train_acc += (pred[:,-1,:].argmax(1) == label_tensor).sum()\n",
    "\n",
    "    cur_train_loss = np.mean(train_losses)\n",
    "    train_loss_logger.append(cur_train_loss)\n",
    "    \n",
    "    train_acc = (train_acc/len(train_y)).item()\n",
    "    train_accuracy_logger.append(train_acc)\n",
    "\n",
    "\n",
    "    sentiment_classifier.eval()\n",
    "    test_acc = 0\n",
    "    test_losses = list()\n",
    "    with torch.inference_mode():\n",
    "        for test_batch in tqdm(test_batches,desc=\"Testing in Batches\",leave=False):\n",
    "            text = test_x[test_batch]\n",
    "            label_tensor = test_y[test_batch]\n",
    "\n",
    "            text = text_transform(text).to(device)\n",
    "\n",
    "            hidden, memory = sentiment_classifier.init_hidden(len(label_tensor),device)\n",
    "            \n",
    "            pred = sentiment_classifier(text,hidden,memory)\n",
    "\n",
    "            loss = loss_fun(pred[:,-1,:],label_tensor)\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            test_acc += (pred[:,-1,:].argmax(1) == label_tensor).sum()\n",
    "\n",
    "    cur_test_loss = np.mean(test_losses)\n",
    "    test_loss_loger.append(cur_test_loss)\n",
    "    test_acc = (test_acc/len(test_y)).item()\n",
    "    test_accuracy_logger.append(test_acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9333298-f98a-4f77-8126-51bcb1db8f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.097343</td>\n",
       "      <td>1.097328</td>\n",
       "      <td>0.336354</td>\n",
       "      <td>0.339016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.099918</td>\n",
       "      <td>1.100476</td>\n",
       "      <td>0.338204</td>\n",
       "      <td>0.365365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.097102</td>\n",
       "      <td>1.096292</td>\n",
       "      <td>0.339850</td>\n",
       "      <td>0.340444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.095729</td>\n",
       "      <td>1.098802</td>\n",
       "      <td>0.339116</td>\n",
       "      <td>0.337587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.009755</td>\n",
       "      <td>0.768440</td>\n",
       "      <td>0.437932</td>\n",
       "      <td>0.637683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.652685</td>\n",
       "      <td>0.569506</td>\n",
       "      <td>0.711592</td>\n",
       "      <td>0.753619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.529243</td>\n",
       "      <td>0.563355</td>\n",
       "      <td>0.775061</td>\n",
       "      <td>0.749460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.471331</td>\n",
       "      <td>0.524910</td>\n",
       "      <td>0.802299</td>\n",
       "      <td>0.777429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.421404</td>\n",
       "      <td>0.529671</td>\n",
       "      <td>0.826680</td>\n",
       "      <td>0.777683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Train Loss  Test Loss  Train Accuracy  Test Accuracy\n",
       "0       1    1.097343   1.097328        0.336354       0.339016\n",
       "1       2    1.099918   1.100476        0.338204       0.365365\n",
       "2       3    1.097102   1.096292        0.339850       0.340444\n",
       "3       4    1.095729   1.098802        0.339116       0.337587\n",
       "4       5    1.009755   0.768440        0.437932       0.637683\n",
       "5       6    0.652685   0.569506        0.711592       0.753619\n",
       "6       7    0.529243   0.563355        0.775061       0.749460\n",
       "7       8    0.471331   0.524910        0.802299       0.777429\n",
       "8       9    0.421404   0.529671        0.826680       0.777683"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance = pd.DataFrame({\"Epochs\": list(range(1,len(train_loss_logger)+1)),\"Train Loss\": train_loss_logger,\"Test Loss\": test_loss_loger,\"Train Accuracy\": train_accuracy_logger, \"Test Accuracy\": test_accuracy_logger})\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdacc7-91cc-4ac6-a0f7-959318bff37d",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Plotting Train and Test Loss\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14c8b424-1efe-492b-8e47-dacc41e466ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_56.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(model_performance,x=\"Epochs\",y=[\"Train Loss\",\"Test Loss\"],markers=True,height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392bb4a-6fd7-486d-8255-63ada0c45ab3",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Plotting Train and Test Accuracy\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f545d64-a179-4005-ab9a-cee3f49d9dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"520\"\n",
       "    src=\"iframe_figures/figure_58.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px.line(model_performance,x=\"Epochs\", y = [\"Train Accuracy\",\"Test Accuracy\"],markers=True,height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4ef07-581e-4bdc-8817-9a6dc2cd932e",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    We are able to achieve â‰ˆ 78% accuracy from our model\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20a0ad81-ff77-4a7d-8639-9c88e26c9847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7776825428009033"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_accuracy_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27774c-062f-4e1e-b598-d456bb6042e9",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Creating a helper function to use our model to perform sentiment analysis\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80366353-4b22-4cf0-8cdb-43412fe785b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review_data):\n",
    "    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    sentiment_classifier.eval()\n",
    "    with torch.inference_mode():\n",
    "        for index, row in review_data.iterrows():\n",
    "            orginal_sentiment = sentiment_map[stars_to_sentiment(row[\"stars\"])]\n",
    "            review_tokens = [lemmatizer.lemmatize(i) for i in simple_preprocess(row[\"text\"])]\n",
    "            review_tokens = np.expand_dims(review_tokens,axis=0).tolist()\n",
    "            review_tensor = text_transform(review_tokens).to(device)\n",
    "            hidden, memory = sentiment_classifier.init_hidden(1,device)\n",
    "            pred = sentiment_classifier(review_tensor,hidden,memory)\n",
    "            pred_sentiment = sentiment_map[pred[:,-1,:].argmax(1).item()]\n",
    "            pred_sentiment_probability = pred[:,-1,:].softmax(1).max().item()\n",
    "\n",
    "            print(\"Review Text:- \")\n",
    "            print(row[\"text\"])\n",
    "            print(\"========================================================================================================\")\n",
    "            print(f\"Rating:- {row[\"stars\"]}\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(f\"Actual Sentiment:- {orginal_sentiment}\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(f\"Predicted Sentimen:- {pred_sentiment}\")\n",
    "            print(f\"Prediction Probability:- {pred_sentiment_probability}\")\n",
    "            print(\"#########################################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabe9af-46ce-49a3-b7a3-bab4c3b09f80",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's fetch some completely unseen data and perform sentiment analysis\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "888a1be3-d97e-4bb6-9ec7-d4ac078ab3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv(\"yelp_review.csv\",usecols=[\"stars\",\"text\"]).sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "828e8a10-8dde-4f44-8fd3-3074526c6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Text:- \n",
      "I wrote the below review on Elegant Smile and soon after learned they changed their name to Gentle Dental.  This place is dishonest.  I recently took my Plan of Care from this office to my new dentist and was told a lot of the work Elegant Smile told me I needed was not warranted.  It's been a few years, I've had none of the work they listed done and have zero problems.  As I stated in my original review below they charged me over $400 for a bill they told me would only be two $50 copays (I have a contract stating this is what would be charged) and have been sent to collections.  I still refuse to pay and it has not hurt my credit whatsoever.  It's a matter of principal.  You had me sign a form acknowledging what I owe and then you charge me four times that amount?  There is very little I dislike more than a dishonest dentist.  \n",
      "\n",
      "Original Review for Elegant Smile:\n",
      "I was told I would have a $50 copay each visit, two visits total for a deep clean,  After my second visit I was told I would have to pay over $400!  I signed a Plan of Care stating I was aware of my $50 copay but apparently they do not honor their own contract. They are now trying to send me to collections. Do not trust them!  I would give them zero stars if it were an option.\n",
      "========================================================================================================\n",
      "Rating:- 1\n",
      "========================================================================================================\n",
      "Actual Sentiment:- negative\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- negative\n",
      "Prediction Probability:- 0.9942498207092285\n",
      "#########################################################################################################\n",
      "Review Text:- \n",
      "My boyfriend and I have been going here for years. Unfortunately, the quality of their food has drastically declined. The waiters are still attentive and pleasant but one of the actual sushi chefs could work on his attitude. While ordering food here, he cut me off and said that is good enough for now and moved on to the next customer. The rice on the roll was hard. I'm positive he was aware of it as well since it would've been impossible for him not to feel the old, hard rice while making the roll. We're not going back to this place. We now go to Doh sushi. It's way bigger and the quality of the food is way better than Yama's.\n",
      "========================================================================================================\n",
      "Rating:- 2\n",
      "========================================================================================================\n",
      "Actual Sentiment:- negative\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- negative\n",
      "Prediction Probability:- 0.9098082780838013\n",
      "#########################################################################################################\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(testing_data[testing_data[\"stars\"] < 3].sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8f5ef8c3-e261-44b3-a86d-3c896411d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Text:- \n",
      "I am still searching for the perfect Thai meal so met up with a buddy for lunch at this place.  The place is neutrally decorated and was fairly empty as it was Labor Day.  We both ordered our favorites: pad thai and yellow curry.  This place runs a lunch special with rice, salad, and won tons.The food was OK, nothing exciting but definitely Americanized.  I wish them well and think they have the potential to do a brisk lunch business due to their location and pricing.  As for me, I'm continuing my search.\n",
      "========================================================================================================\n",
      "Rating:- 3\n",
      "========================================================================================================\n",
      "Actual Sentiment:- neutral\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- neutral\n",
      "Prediction Probability:- 0.849919855594635\n",
      "#########################################################################################################\n",
      "Review Text:- \n",
      "This was my first time at the Wicked Spoon Buffet and I was super excited to see what it was all about. $35 bucks for dinner during the weekend. \n",
      "\n",
      "Definitely 5 stars for the decor, ambiance, food presentation and service at this place - it is quite uncommon for a buffet to have individualized portions for their selections of food - mini sized pots...super cute & creative! I was definitely in awe when I saw the decoration and presentation of food from the moment I walked in. \n",
      "\n",
      "Like any other buffets, I always browse the selection of food choices one round before preparing my first plate. Everything looked delicious - even something as simple as Cesar  salad was served in small portions. \n",
      "\n",
      "However, the food that day was just mediocre so I'd give it a 3.5 stars overall. \n",
      "\n",
      "CONS: I expected prime rib like I saw on yelp but it wasn't served that day. Instead, they had the new york steak which was a bit on the dry side and kind of tough to eat. No accompanying sauce was provided to provide the meat any moisture. I also tried the BBQ pork which I did not enjoy, the first bite left a really strong aftertaste...I didn't finish my slice. The Asian portion of the buffet was just okay - nothing you wouldn't find at a typical Chinese restaurant or can make at home! Fried rice, orange chicken, crispy chow mein, and stir fried clams (wasn't a fan of their sauce). The other item from here was the fried frog legs - I personally don't know how this tasted b\\/c I contemplated on trying it. Sushi selection was limited and simple - California rolls, tempura rolls and a veggie roll. Ohh and DO NOT even try the so called \"pho\" - they used the wrong noodles & the broth tasted so watered-down. My bowl was left untouched after the first spoonful. \n",
      "\n",
      "PROS: Their salad selection and deli section was pretty good. A lot of variety to choose from which was different from other buffets on the strip. I particular enjoyed and remembered the spicy shrimp salad - it was very savory and flavorful. Their dessert selection was very impressive. A lot of sweets to choose from - chocolate covered strawberries, ice cream, petite cakes, cookies, etc. I was looking forward to trying the macacrons but too bad they didn't have it the day I was there. \n",
      "\n",
      "Overall, I was full but wasn't fully satisfied after eating here. Maybe my second time here would be a better experience. I wished the food selection was more consistent so that anyone's first experience would be the same. I would come back for a second try when I visit again! \n",
      "\n",
      "Gourmet presentation but lacks gourmet food selection. \n",
      "\n",
      "Happy Eating =)\n",
      "========================================================================================================\n",
      "Rating:- 3\n",
      "========================================================================================================\n",
      "Actual Sentiment:- neutral\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- neutral\n",
      "Prediction Probability:- 0.8827182054519653\n",
      "#########################################################################################################\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(testing_data[testing_data[\"stars\"] == 3].sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1666f9e-b1ae-48f5-85ec-b4f81efd95b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Text:- \n",
      "Love it! Great beer and awesome service. Glad to have this gem within walking distance.\n",
      "========================================================================================================\n",
      "Rating:- 5\n",
      "========================================================================================================\n",
      "Actual Sentiment:- positive\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- positive\n",
      "Prediction Probability:- 0.9656999707221985\n",
      "#########################################################################################################\n",
      "Review Text:- \n",
      "Awesome brunch option! Will go back for dinner for sure! Came here with the whole fam and everyone was impressed. I'm not a huge brunch guy (we went for lunch) but when I looked at the brunch menu I was intrigued...and I'm glad I tried it...really cool twist on eggs Benedict with Arepas in place of the English muffins. Can't explain how good this was. Kids had lunch quesadillas and wife had a Chimichanga everything was awesome...we'll be back for sure.\n",
      "========================================================================================================\n",
      "Rating:- 5\n",
      "========================================================================================================\n",
      "Actual Sentiment:- positive\n",
      "========================================================================================================\n",
      "Predicted Sentimen:- positive\n",
      "Prediction Probability:- 0.9520800709724426\n",
      "#########################################################################################################\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(testing_data[testing_data[\"stars\"] > 3].sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ef8ed-95e5-439e-a8a8-f4e39764a77d",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Finally with above result we can conclude our analysis. Our model was able to get good accuracy on the testing dataset and also able to correctly identify the sentiment of the unseen datas.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a48d5-06e3-4b39-853c-1dde70a9ec7e",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Saving our trained model for future use.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a89cd6e7-f4c3-4d08-92e3-8c33a4786760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSTM_Sentiment_Classifier.pkl']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sentiment_classifier,\"LSTM_Sentiment_Classifier.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
